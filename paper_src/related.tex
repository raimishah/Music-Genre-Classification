\section{Related Work}\label{sec:related}


{\em Query processing}.  Several studies are presented for making query processing and database operators aware of on-chip cache spaces in the context of both single core machines \cite{Shatdal:1994} and multicore architectures \cite{Cieslewicz:2007,Cieslewicz:2009,Yadan:2009}.  Acker et al \cite{Acker:2008}  present an approach that encapsulate parallelism for relational database query execution, which  strives for maximum resource utilization for both CPU and disk activities.  Liknes \cite{Stian} investigates  database algorithms and methods for modern multi-core processors in main memory environments. Stonebraker et al. \cite{Stonebraker:2005} and Boncz et al. \cite{Boncz:2008} introduce tuple access and storage optimizations in order to cope with the memory access bottleneck.  Albutiu et al \cite{Albutiu:2012} devise a suite of new massively parallel sort-merge (MPSM) join algorithms that are based on partial partition-based sorting.  These MPSM algorithms are NUMA-affine as all the sorting is carried out on local memory partitions.    Duffy and Essey \cite{Joe}  review the goals of the PLINQ technology, where it fits into the broader .NET Framework and other concurrency offerings.  Lee et al. \cite{Lee:2009} specifically target database queries sharing same on-chip cache structures in multicore architectures. They introduce an OS-level cache partitioning scheme which is based on data access patterns and working memory requirements of the given workload queries. 
 As compared to these works, our approach considers a batch of queries running concurrently (instead of a single query) and exploits data locality opportunities in a global fashion. 
 
{\em Data sharing}. Harizopoulos et al. \cite{Harizopoulos:2005} present a pipelined query engine where a single data retrieval operation serves more than one query in parallel.  Petrides et al \cite{Petrides:2013SED}  propose different representative data-parallel versions of the original database scan and join algorithms to exploit the benefits of using on-chip clustered many-core architectures, and study the impact on the performance when on-chip memory, shared among all cores, is used as a prefetching buffer.  In \cite{Ross:2010,Qiao:2008,Zukowski:2007}, work sharing opportunities through exploiting common operators across concurrently-running queries are discussed. The goal of our multi-query scheduling scheme is similar to these work sharing approaches from the data locality perspective. However, we focus more on the issues arising due to shared caches and different on-chip cache topologies. Extending our approach with these expert work sharing based approaches can further improve data locality through all levels of the on-chip cache hierarchies in multicores.  

In addition, batch scheduling and resource allocation problems have been studied in the scope of parallel database systems \cite{Yu:1986,Mehta:1993}. In comparison, our work specifically targets emerging multicore platforms and focuses on the problem of optimizing data locality in shared on-chip cache hierarchies.

%{\em Cache partitioning} \cite{Kim:2004,Lu:2009,Rafique:2006,Lee:2009,Chang:2007}. In general, one can take advantage of sharing and/or isolation by explicitly partitioning shared caches, and as a result, minimize possible cache conflicts in multi-threaded environments. Lee et al. \cite{Lee:2009} specifically target database queries sharing same on-chip cache structures in multicore architectures. They introduce an OS-level cache partitioning scheme which is based on data access patterns and working memory requirements of the given workload queries. 

%Our approach is complementary to prior studies on query processing and data-sharing optimizations for shared cache architectures. Supporting such prior approaches with our scheduling scheme can further improve query latencies and overall system throughput. Further, cache partitioning schemes can favor our affinity domain-query mapping based scheduling scheme, especially when constructive cache isolation for mappings cannot be provided on a specific topology by default. %They can introduce alternative affinity domains to existing hardware directed cache structures and provide them as input to our multi-query scheduler. 

